---
title: Perform Rolling Update Deployments
categories:
- xl-deploy
subject:
- Deployment
tags:
- rolling update
- deployment pattern
- load balancer
---

This guide explains how to perform the Rolling Update deployment pattern using XL Deploy.

XL Deploy's powerful [orchestrator feature](/xl-deploy/concept/types-of-orchestrators-in-xl-deploy.html) will take care of calculating the correct deployment plan, providing support for a scalable solution. No scripting is needed, only configuration of the environments, load balancer and application.

In the Rolling Update pattern, the application is run on several nodes. Traffic to these nodes is distributed by a load balancer. When updating to a new version, one node is taken out of the load balancer pool at the time and taken offline to update. This ensures that the application is still available because it is being served by the other nodes. When the update is complete, the updated node is added to the load balancer pool again and the next node is updated, until all nodes are done.

This pattern requires that at least two versions of the software are active in the same environment at the same time. This adds requirements to the architecture of the software, for example: the two versions must be able to connect to the same database and the database upgrades must be managed more carefully. This is outside the scope of this article. <!-- ADD SOME LINK -->

*Note: this guide was written using XL Deploy 7.6.*



## Import sample application

The Rolling Update deployment pattern can be used with any application.

In this example we will use the PetClinic demo application that is shipped with XL Deploy. To import it, use the context menu on **Applications** to import it from **XL Deploy Server**. Import both versions 1.0 and 2.0.

![Create new release](images/rolling-update/import-petclinic.png)

## Preparing the nodes 

First we will set up the multiple nodes that will serve the application and make sure that they are updated in the correct order.

For the Rolling Update deployment pattern we will use the [Deployment Group Orchestrator](/xl-deploy/concept/types-of-orchestrators-in-xl-deploy.html#by-deployment-group-orchestrators). With this orchestrator, you group containers that belong together and assign each group a number. XL Deploy will generate a deployment plan that deploys the application group by group in the order specified.

Let's define the infrastructure nodes that will run the application. In this example we will use an application deployed to Apache Tomcat, but keep in mind that his technique applies to any setup.

In this example we have three application servers that will host our application simultaneously.

![Create new release](images/rolling-update/appserver-infrastructure.png)

We will deploy the application to **Tomcat 1**, **Tomcat 2** and **Tomcat 3**.

Steps to create this infrastructure:

1. For **Appserver Host**, create an Overthere host like `overthere.SshHost` that connects to the physical machine running the tomcat installations.
2. For each **Appserver**, create a `tomcat.Server` that points to the Tomcat installation directory.
3. To create the **Tomcat** targets, add a `tomcat.VirtualHost`. 

Each Tomcat server get its own deployment group, so we can deploy in sequence.

To do so, edit each Tomcat server, find the 'Deployment' section and fill in the sequence number of the rolling update. Note that the Deployment section is available on any [container](https://docs.xebialabs.com/xl-deploy/concept/key-xl-deploy-concepts.html#containers) in XL Deploy, not only Tomcat hosts.

![Create new release](images/rolling-update/deployment-group-number.png)

Now, under **Environments**, create an environment **Rolling Environment** and add the three Tomcat Servers to it.

![Create new release](images/rolling-update/rolling-environment.png)

You are now ready to do your first rolling deployment!

## First rolling deployment

Start a deployment of **PetClinic 1.0** by selecting **Deploy** from the context menu and choosing the **Rolling Environment** you just created.

In the **Configure** screen, press the **Preview** button to see the deployment plan that will be generated by XL Deploy.

As we can see when we expand the plan, the plan is not a 'rolling update'. All servers are stopped, then the application is deployed to all three servers and then all three servers are started again. Hardly a rolling update!

This is where the orchestrators and the magic of XL Deploy's AutoFlow come in.

Click on **Deployment Properties** on the top-left side of the screen.

In the Orchestrator field, type `sequential-by-deployment-group` and press **Add**. 

![Add sequential-by-deployment-group orchestrator](images/rolling-update/sequential-by-deployment-group.png)

The orchestrators take care of modifying the plan. In this case, the ** sequential-by-deployment-group** orchestrator will create a rolling deployment plan. As we will see later on, it is possible to stack orchestrators to create fine tuned, scalable deployment plans.

When we close the dialog, the plan is immediately updated.

![Plan modified by orchestrator](images/rolling-update/plan-modification.png)

You can now run the deployment.

In principle, this is enough to do a rolling update deployment at any scale. Just configure the groups and add the proper orchestrator.

In the next section we will see how to add a load balancer to the mix and how to select the required orchestrators by default so no manual plan modification is needed.

## Adding the load balancer

A load balancer is essential for the rolling update deployment pattern. While one node is being upgraded, it should not receive any traffic. The load balancer should take care of this, routing traffic to the other nodes while one is down for the upgrade.

XL Deploy supports a number of load balancers, available as plugins. In this example we will use the
[F5 BigIp plugin](https://docs.xebialabs.com/xl-deploy/concept/f5-big-ip-plugin.html), but the principle is the same for all load balancer plugins.

For BigIp, add an Overthere host and a `big-ip.LocalTrafficManager` to the Infrastructure. Configure the CIs according to the instructions of the load balancer plugin documentation.

For our example we end up with the following infrastructure:

![Infrastructure with load balancer](images/rolling-update/infrastructure-with-loadbalancer.png)

On the load balancer, add the nodes we are deploying to on the **Managed Servers** property. We are using the F5 BigIp plugin, but this property is available on any load balancer plugin.

![Managed servers on the load balancer](images/rolling-update/managed-servers.png)

Add load balancer itself to the environment. In our case the **Traffic Manager** is added to the **Rolling Environment**.

![Environment with load balancer](images/rolling-update/environment-with-loadbalancer.png)

Now we are ready to start the deployment again. In order to trigger the load balancing behavior in the plan, we need to add another orchestrator: `sequential-by-loadbalancer-group`.

![Plan with load balancer](images/rolling-update/deployment-properties-with-loadbalancer.png)

The resulting plan takes the load balancer into account and removes and adds the Tomcat servers from the load balancer when the node is being upgraded.

![Plan with load balancer](images/rolling-update/plan-with-loadbalancer.png)

The plan is now ready for a rolling update deployment with zero downtime.

## Preparing the applications for the Rolling Update deployment pattern

The plan is perfect, but there is still a manual step: adding the orchestrators to the Deployment Properties when creating the deployment. 

There are two ways to configure the CIs the pick up the orchestrators automatically.

### Setting orchestrators on the application

The easiest way to configure orchestrators automatically is to configure them directly on the application to be deployed. This is the most convenient way if the rolling update pattern applies to all environments the application is deployed to.

Open the deployment package, say **PetClinic/1.0** in the Explorer and add the relevant orchestrators to the **Orchestrator** property.

![Orcherstrators on deployment package](images/rolling-update/orchestrators-on-deployment-package.png)

The disadvantage is that the orchestrators are hardcoded on the application and may not apply to each environment. For example, rolling update is only needed in the Production environment but not in the QA environment.

### Setting orchestrators on the environment using a dictionary

It is not possible to set orchestrators on the environment directly, but we can work around this be using dictionaries.

First, remove the orchestrator from the PetClinic application.

Now add create a dictionary called **Dictionary**.

![Plan with load balancer](images/rolling-update/add-dictionary.png)

In the dictionary, create the following entry:

```
Key                                    Value
udm.DeployedApplication.orchestrator   sequential-by-deployment-group, sequential-by-loadbalancer-group
```

We are using two dictionary tricks here

 * The key maps to a fully quantified property of the application being deployed. If this property is left empty on the application, the value is taken from the dictionary.
* The value is a comma-separated list and will be mapped to a list of values.

Add the dictionary to **Rolling Environment**.

Now start the deployment again. The orchestrators should be picked up, generating the correct plan without having to configure anything on the application directly.

## Summary

In this article we have shown how to perform a rolling update in XL Deploy without scripting. This can be done by installing a load balancer plugin and configuring the `sequential-by-group` and `sequential-by-loadbalancer-group` orchestrators.

More orchestrators can still be added to fine tune the generated deployment plan.

This is a scalable approach that will work for any environment or any number of applications or environments.